{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logs = sc.textFile(\"hdfs://l41-srv-mcdh01.b.internal/datasets/magichour/tbird.log.gz\").repartition(30)\n",
    "simpleTransformations = sc.textFile(\"hdfs://l41-srv-mcdh01.b.internal/magichour/simpleTrans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#logs.count()\n",
    "#simpleTransformations.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import re\n",
    "\n",
    "TransformLine = namedtuple('TransformLine',['id','type','NAME','transform','compiled'])\n",
    "LogLine = namedtuple('LogLine', ['ts','msg','processed','dictionary','supportId'])\n",
    "\n",
    "def get_Transforms(line):\n",
    "    if line.lstrip()[0] != '#':\n",
    "        l = line.lstrip().rstrip().split(',', 3)  #id,type,name,transform\n",
    "        return TransformLine(int(l[0]),l[1],l[2],l[3],re.compile(l[3]))\n",
    "    else:\n",
    "        return TransformLine('COMMENT','COMMENT','COMMENT','COMMENT','COMMENT')\n",
    "\n",
    "logTransforms = simpleTransformations.map(get_Transforms).cache()\n",
    "#logTransformList = sc.broadcast(list(simpleTransformations))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# logTransforms.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = logTransforms.collect()\n",
    "lTrans = list()\n",
    "\n",
    "for l in a:\n",
    "    if l.id != 'COMMENT':\n",
    "        lTrans.append(l)\n",
    "        \n",
    "logTrans= sc.broadcast(lTrans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def makeTransformedLine(l):\n",
    "    '''\n",
    "    apply a list of regex replacements to a line, make note of\n",
    "    all the remplacements peformed in a dictionary(list)\n",
    "\n",
    "    Args:\n",
    "        l(LogLine): logline to work on\n",
    "        transforms(list(TransformLine)): replacemnts to make with\n",
    "\n",
    "    Returns:\n",
    "        retval(LogLine): logline with the processed, and dictionary portions\n",
    "                         filled in\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    text = l.msg.strip()\n",
    "    replaceDict = dict()\n",
    "    # logTrans is broadcast\n",
    "    for t in logTrans.value:\n",
    "        if t.type == 'REPLACE':\n",
    "            replaceList = t.compiled.findall(text)\n",
    "            if replaceList:\n",
    "                replaceDict[t.NAME] = replaceList\n",
    "            text = t.compiled.sub(t.NAME, text, 0)\n",
    "\n",
    "        if t.type == 'REPLACELIST':\n",
    "            print 'REPLACELIST not implemented yet'\n",
    "\n",
    "    processed = ' '.join(text.split())\n",
    "    retVal = LogLine(l.ts, l.msg.lstrip().rstrip(),\n",
    "                     processed.lstrip().rstrip(), replaceDict, None)\n",
    "\n",
    "    return retVal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tsLine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-cf230a544f8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpotato\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtsLine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmakeTransformedLine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tsLine' is not defined"
     ]
    }
   ],
   "source": [
    "potato = tsLine.map(makeTransformedLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "potato.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "potato.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import re\n",
    "\n",
    "TransformLine = namedtuple('TransformLine',\n",
    "                           ['id', 'type', 'NAME', 'transform', 'compiled'])\n",
    "\n",
    "LogLine = namedtuple('LogLine', ['ts', 'msg',\n",
    "                                 'processed', 'dictionary', 'supportId'])\n",
    "\n",
    "\n",
    "def rdd_TransformLine(line):\n",
    "    '''\n",
    "    process transformations into RDD format\n",
    "\n",
    "    Args:\n",
    "        line(string): line from the transform defintion file.\n",
    "                      lines beginning with # are considered comments\n",
    "                      and will need to be removed\n",
    "    Returns:\n",
    "        retval(TransformLine): namedTuple representation of the tasking\n",
    "    '''\n",
    "\n",
    "    if line.lstrip()[0] != '#':\n",
    "        # id,type,name,transform\n",
    "        l = line.lstrip().rstrip().split(',', 3)\n",
    "        return TransformLine(int(l[0]), l[1], l[2], l[3], re.compile(l[3]))\n",
    "    else:\n",
    "        return TransformLine('COMMENT',\n",
    "                             'COMMENT',\n",
    "                             'COMMENT',\n",
    "                             'COMMENT',\n",
    "                             'COMMENT')\n",
    "\n",
    "\n",
    "def rdd_LogLine(line):\n",
    "    '''\n",
    "    process a log line into a RDD\n",
    "\n",
    "    Args:\n",
    "        line(string): string from the logline\n",
    "\n",
    "    Returns:\n",
    "        retval(LogLine): fills in the first two portions of the LogLine\n",
    "                         namedtuple\n",
    "    '''\n",
    "\n",
    "    # depends on tbird log structure\n",
    "    l = line.strip().rstrip().split(' ', 3)\n",
    "    return LogLine(float(l[2]), l[3], None, None, None)\n",
    "\n",
    "\n",
    "def lineRegexReplacement(line):\n",
    "    '''\n",
    "    apply a list of regex replacements to a line, make note of\n",
    "    all the remplacements peformed in a dictionary(list)\n",
    "\n",
    "    Args:\n",
    "        line(LogLine): logline to work on\n",
    "\n",
    "    Globals:\n",
    "        transforms(RDD(TransformLine)): replacemnts to make with\n",
    "\n",
    "    Returns:\n",
    "        retval(LogLine): logline with the processed, and dictionary portions\n",
    "                         filled in\n",
    "    '''\n",
    "    global logTrans\n",
    "    text = line.msg.strip()\n",
    "    replaceDict = dict()\n",
    "\n",
    "    # logTrans is broadcast\n",
    "    for t in logTrans.value:\n",
    "        if t.type == 'REPLACE':\n",
    "            replaceList = t.compiled.findall(text)\n",
    "            if replaceList:\n",
    "                replaceDict[t.NAME] = replaceList\n",
    "            text = t.compiled.sub(t.NAME, text, 0)\n",
    "\n",
    "        if t.type == 'REPLACELIST':\n",
    "            print 'REPLACELIST not implemented yet'\n",
    "\n",
    "    processed = ' '.join(text.split())\n",
    "    retVal = LogLine(line.ts, line.msg.lstrip().rstrip(),\n",
    "                     processed.lstrip().rstrip(), replaceDict, None)\n",
    "\n",
    "    return retVal\n",
    "\n",
    "\n",
    "def readTransforms(transFile):\n",
    "\n",
    "    # map the transFile\n",
    "    simpleTransformations = sc.textFile(transFile)\n",
    "\n",
    "    # parse loglines\n",
    "    return simpleTransformations.map(rdd_TransformLine).cache()\n",
    "\n",
    "   \n",
    "\n",
    "def doPreProcess(logFile, transFile, partitions):\n",
    "    '''\n",
    "        take a series of loglines and pre-process the lines\n",
    "        replace ipaddresses, directories, urls, etc with constants\n",
    "        keep a dictionary of the replacements done to the line\n",
    "\n",
    "        Args:\n",
    "            logFile(string): location of the log data in HDFS\n",
    "            transFile(string): location of the replacement regex in HDFS\n",
    "            partitions(int): number of partitions to apply to the logFile\n",
    "\n",
    "        Returns:\n",
    "            retval(RDD(LogLines)): preprocessed log lines ready for next\n",
    "                                   stage of processing\n",
    "   '''\n",
    "\n",
    "    # read the logs\n",
    "    logs = sc.textFile(logFile).repartition(partitions)\n",
    "\n",
    "    # read the transforms, removing comments\n",
    "    logTransforms = readTransforms(transFile)\n",
    "\n",
    "    trans = logTransforms.collect()\n",
    "    lTrans = list()\n",
    "\n",
    "    for t in trans:\n",
    "        if t.id != 'COMMENT':\n",
    "            lTrans.append(t)\n",
    "\n",
    "    logTrans = sc.broadcast(lTrans)\n",
    "\n",
    "    \n",
    "    tsLine = logs.map(rdd_LogLine)\n",
    "    return tsLine.map(lineRegexReplacement)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logs = 'hdfs://l41-srv-mcdh01.b.internal/datasets/magichour/tbird.log.gz'\n",
    "logs = 'hdfs://l41-srv-mcdh01.b.internal/user/dgrossman/tbird.log.10000.gz'\n",
    "trans = 'hdfs://l41-srv-mcdh01.b.internal/magichour/simpleTrans'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = doPreProcess(logs,trans,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 4 times, most recent failure: Lost task 0.3 in stage 15.0 (TID 35, 10.161.0.19): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/cdh_data/mesos/slaves/20150827-215802-671129866-5050-2150-S19/frameworks/20150827-215802-671129866-5050-2150-0255/executors/13/runs/1f9864d2-de6b-4432-8b3a-7a70e326bce4/spark-1.6.0-bin-hadoop2.4/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/cdh_data/mesos/slaves/20150827-215802-671129866-5050-2150-S19/frameworks/20150827-215802-671129866-5050-2150-0255/executors/13/runs/1f9864d2-de6b-4432-8b3a-7a70e326bce4/spark-1.6.0-bin-hadoop2.4/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/cdh_data/mesos/slaves/20150827-215802-671129866-5050-2150-S19/frameworks/20150827-215802-671129866-5050-2150-0255/executors/13/runs/1f9864d2-de6b-4432-8b3a-7a70e326bce4/spark-1.6.0-bin-hadoop2.4/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark-1.6.0-bin-hadoop2.4/python/pyspark/rdd.py\", line 1293, in takeUpToNumLeft\n  File \"<ipython-input-6-aa487168820b>\", line 70, in makeTransformedLine\nNameError: global name 'logTrans' is not defined\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:724)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:393)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/cdh_data/mesos/slaves/20150827-215802-671129866-5050-2150-S19/frameworks/20150827-215802-671129866-5050-2150-0255/executors/13/runs/1f9864d2-de6b-4432-8b3a-7a70e326bce4/spark-1.6.0-bin-hadoop2.4/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/cdh_data/mesos/slaves/20150827-215802-671129866-5050-2150-S19/frameworks/20150827-215802-671129866-5050-2150-0255/executors/13/runs/1f9864d2-de6b-4432-8b3a-7a70e326bce4/spark-1.6.0-bin-hadoop2.4/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/cdh_data/mesos/slaves/20150827-215802-671129866-5050-2150-S19/frameworks/20150827-215802-671129866-5050-2150-0255/executors/13/runs/1f9864d2-de6b-4432-8b3a-7a70e326bce4/spark-1.6.0-bin-hadoop2.4/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark-1.6.0-bin-hadoop2.4/python/pyspark/rdd.py\", line 1293, in takeUpToNumLeft\n  File \"<ipython-input-6-aa487168820b>\", line 70, in makeTransformedLine\nNameError: global name 'logTrans' is not defined\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:724)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-7ca03adfea90>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/spark-1.6.0-bin-hadoop2.4/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1296\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1297\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark-1.6.0-bin-hadoop2.4/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m    937\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 939\u001b[1;33m         \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    940\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark-1.6.0-bin-hadoop2.4/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark-1.6.0-bin-hadoop2.4/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark-1.6.0-bin-hadoop2.4/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    307\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 4 times, most recent failure: Lost task 0.3 in stage 15.0 (TID 35, 10.161.0.19): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/cdh_data/mesos/slaves/20150827-215802-671129866-5050-2150-S19/frameworks/20150827-215802-671129866-5050-2150-0255/executors/13/runs/1f9864d2-de6b-4432-8b3a-7a70e326bce4/spark-1.6.0-bin-hadoop2.4/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/cdh_data/mesos/slaves/20150827-215802-671129866-5050-2150-S19/frameworks/20150827-215802-671129866-5050-2150-0255/executors/13/runs/1f9864d2-de6b-4432-8b3a-7a70e326bce4/spark-1.6.0-bin-hadoop2.4/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/cdh_data/mesos/slaves/20150827-215802-671129866-5050-2150-S19/frameworks/20150827-215802-671129866-5050-2150-0255/executors/13/runs/1f9864d2-de6b-4432-8b3a-7a70e326bce4/spark-1.6.0-bin-hadoop2.4/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark-1.6.0-bin-hadoop2.4/python/pyspark/rdd.py\", line 1293, in takeUpToNumLeft\n  File \"<ipython-input-6-aa487168820b>\", line 70, in makeTransformedLine\nNameError: global name 'logTrans' is not defined\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:724)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:393)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/cdh_data/mesos/slaves/20150827-215802-671129866-5050-2150-S19/frameworks/20150827-215802-671129866-5050-2150-0255/executors/13/runs/1f9864d2-de6b-4432-8b3a-7a70e326bce4/spark-1.6.0-bin-hadoop2.4/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/home/cdh_data/mesos/slaves/20150827-215802-671129866-5050-2150-S19/frameworks/20150827-215802-671129866-5050-2150-0255/executors/13/runs/1f9864d2-de6b-4432-8b3a-7a70e326bce4/spark-1.6.0-bin-hadoop2.4/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/home/cdh_data/mesos/slaves/20150827-215802-671129866-5050-2150-S19/frameworks/20150827-215802-671129866-5050-2150-0255/executors/13/runs/1f9864d2-de6b-4432-8b3a-7a70e326bce4/spark-1.6.0-bin-hadoop2.4/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/local/spark-1.6.0-bin-hadoop2.4/python/pyspark/rdd.py\", line 1293, in takeUpToNumLeft\n  File \"<ipython-input-6-aa487168820b>\", line 70, in makeTransformedLine\nNameError: global name 'logTrans' is not defined\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:724)\n"
     ]
    }
   ],
   "source": [
    "out.take(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
