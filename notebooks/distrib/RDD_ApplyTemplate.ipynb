{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timeit():\n",
    "    time.ctime()\n",
    "    return time.strftime('%l:%M%p %Z on %b %d, %Y')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timeit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import json\n",
    "\n",
    "LogLine = namedtuple('LogLine', ['ts', 'msg',\n",
    "                                 'processed', 'dictionary', \n",
    "                                 'template','templateId','templateDict'])\n",
    "\n",
    "TemplateLine = namedtuple('TemplateLine',['id','template','skipWords'])\n",
    "\n",
    "\n",
    "TransformLine = namedtuple('TransformLine',\n",
    "                           ['id', 'type', 'NAME', 'transform', 'compiled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frequentWords = json.load(open('freq_word_dict.json'))\n",
    "goodTemplates = json.load(open('templates.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "readLog_RDD.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def procLogLine(line, logFile):\n",
    "    '''\n",
    "    handles the logfile specific parsing input lines into 2 parts\n",
    "    ts: timestamp float\n",
    "    msg: the rest of the message\n",
    "\n",
    "    Args:\n",
    "        line(string): text to process\n",
    "        logFile(string): hint of URI used for input\n",
    "                         should use for switching parsing\n",
    "                         based off different directories\n",
    "\n",
    "    Returns:\n",
    "        retval(list[string,string]): [ts, msg]\n",
    "    '''\n",
    "    return line.strip().rstrip().split(' ', 3)[2:]\n",
    "\n",
    "\n",
    "def rdd_LogLine(line, logFile):\n",
    "    '''\n",
    "    process a log line into a RDD\n",
    "\n",
    "    Args:\n",
    "        line(string): string from the logline\n",
    "        logFile(string): what URI the log lines came from,\n",
    "                         eventually want to do different parsing\n",
    "                         based on the base of the URI\n",
    "\n",
    "    Returns:\n",
    "        retval(LogLine): fills in the first two portions of the LogLine\n",
    "                         namedtuple\n",
    "    '''\n",
    "    l = procLogLine(line, logFile)\n",
    "    return LogLine(float(l[0]),\n",
    "                   l[1],\n",
    "                   None,\n",
    "                   None,\n",
    "                   None,\n",
    "                   None,\n",
    "                   None)\n",
    "\n",
    "\n",
    "def rdd_ReadLog(sc, logFile):\n",
    "    '''\n",
    "    read a log/directory into LogLine RDD format\n",
    "    NOTE: only ts, and msg are populated\n",
    "    Args:\n",
    "        sc(sparkContext)\n",
    "        logFile(string): URI to file toprocess\n",
    "\n",
    "    Returns:\n",
    "        retval(RDD(LogLines): RDD of logs read from the LogFile URI\n",
    "    '''\n",
    "    sparkLogFile = sc.textFile(logFile)\n",
    "\n",
    "    return sparkLogFile.map(lambda line: rdd_LogLine(line, logFile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preProcess_RDD.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rdd_TransformLine(line):\n",
    "    '''\n",
    "    process transformations into RDD format\n",
    "\n",
    "    Args:\n",
    "        line(string): line from the transform defintion file.\n",
    "                      lines beginning with # are considered comments\n",
    "                      and will need to be removed\n",
    "    Returns:\n",
    "        retval(TransformLine): namedTuple representation of the tasking\n",
    "    '''\n",
    "\n",
    "    if line.lstrip()[0] != '#':\n",
    "        # id,type,name,transform\n",
    "        l = line.lstrip().rstrip().split(',', 3)\n",
    "        return TransformLine(int(l[0]),\n",
    "                             l[1],\n",
    "                             l[2],\n",
    "                             l[3],\n",
    "                             re.compile(l[3]))\n",
    "    else:\n",
    "        return TransformLine('COMMENT',\n",
    "                             'COMMENT',\n",
    "                             'COMMENT',\n",
    "                             'COMMENT',\n",
    "                             'COMMENT')\n",
    "\n",
    "def lineRegexReplacement(line, logTrans):\n",
    "    '''\n",
    "    apply a list of regex replacements to a line, make note of\n",
    "    all the remplacements peformed in a dictionary(list)\n",
    "\n",
    "    Args:\n",
    "        line(LogLine): logline to work on\n",
    "\n",
    "    Globals:\n",
    "        transforms(RDD(TransformLine)): replacemnts to make with\n",
    "\n",
    "    Returns:\n",
    "        retval(LogLine): logline with the processed, and dictionary portions\n",
    "                         filled in\n",
    "    '''\n",
    "\n",
    "    text = line.msg.strip()\n",
    "    replaceDict = dict()\n",
    "\n",
    "    for t in logTrans.value:\n",
    "        if t.type == 'REPLACE':\n",
    "            replaceList = t.compiled.findall(text)\n",
    "            if replaceList:\n",
    "                replaceDict[t.NAME] = replaceList\n",
    "            text = t.compiled.sub(t.NAME, text, 0)\n",
    "\n",
    "        if t.type == 'REPLACELIST':\n",
    "            print 'REPLACELIST not implemented yet'\n",
    "\n",
    "    processed = ' '.join(text.split())\n",
    "    retVal = LogLine(line.ts, line.msg.lstrip().rstrip(),\n",
    "                     processed.lstrip().rstrip(), replaceDict, None, None, None)\n",
    "\n",
    "    return retVal\n",
    "\n",
    "\n",
    "def readTransforms(sc, transFile):\n",
    "    '''\n",
    "    returns a list of transforms for replacement processing\n",
    "\n",
    "    Args:\n",
    "        sc(sparkContext): spark context\n",
    "        transFile(string): uri to the transform file in HDFS\n",
    "\n",
    "    Returns:\n",
    "        retval(list(TransformLine))\n",
    "    '''\n",
    "\n",
    "    # map the transFile\n",
    "    simpleTransformations = sc.textFile(transFile)\n",
    "\n",
    "    # parse loglines\n",
    "    logTransforms = simpleTransformations.map(rdd_TransformLine).cache()\n",
    "\n",
    "    trans = logTransforms.collect()\n",
    "\n",
    "    lTrans = list()\n",
    "\n",
    "    for t in trans:\n",
    "        if t.id != 'COMMENT':\n",
    "            lTrans.append(t)\n",
    "\n",
    "    return lTrans\n",
    "\n",
    "\n",
    "def logPreProcess(sc, logTrans, rrdLogLine):\n",
    "    '''\n",
    "    take a series of loglines and pre-process the lines\n",
    "    replace ipaddresses, directories, urls, etc with constants\n",
    "    keep a dictionary of the replacements done to the line\n",
    "\n",
    "    Args:\n",
    "        sc(sparkContext): spark context\n",
    "        logTrans(string): location fo the transFile in HDFS\n",
    "        logFile(string): location of the log data in HDFS\n",
    "\n",
    "    Returns:\n",
    "        retval(RDD(LogLines)): preprocessed log lines ready for next\n",
    "                               stage of processing\n",
    "   '''\n",
    "\n",
    "    # following done to make sure that the broadcast gets to the function\n",
    "    return rrdLogLine.map(lambda line: lineRegexReplacement(line, logTrans))\n",
    "\n",
    "\n",
    "def rdd_preProcess(sc, logTrans, rrdLogLine):\n",
    "    '''\n",
    "    make a rdd of preprocessed loglines\n",
    "\n",
    "    Args:\n",
    "            sc(sparkContext): sparkContext\n",
    "            logTrans(string): location fo the transFile in HDFS\n",
    "            logFile(string): location of the log data in HDFS\n",
    "\n",
    "    Returns:\n",
    "            retval(RDD(LogLines)): preprocessed log lines ready for next\n",
    "                                   stage of processing\n",
    "    '''\n",
    "\n",
    "    lTrans = readTransforms(sc, logTrans)\n",
    "    logTrans = sc.broadcast(lTrans)\n",
    "    return logPreProcess(sc, logTrans, rrdLogLine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rddTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def badWay(r1, r2):\n",
    "    '''\n",
    "    correct way:\n",
    "\n",
    "    For each pair of regexes r and s for languages L(r) and L(s)\n",
    "    Find the corresponding Deterministic Finite Automata M(r) and M(s)   [1]\n",
    "    Compute the cross-product machine M(r x s) and assign accepting states\n",
    "    so that it computes L(r) - L(s)\n",
    "    Use a DFS or BFS of the the M(r x s) transition table to see if any\n",
    "    accepting state can be reached from the start state\n",
    "    If no, you can eliminate s because L(s) is a subset of L(r).\n",
    "    Reassign accepting states so that M(r x s) computes L(s) - L(r)\n",
    "    Repeat the steps above to see if it's possible to eliminate r\n",
    "    '''\n",
    "\n",
    "    return(len(r2)-len(r1))\n",
    "\n",
    "\n",
    "def rankMatches(m):\n",
    "    retval = sorted(m, cmp=badWay)\n",
    "    return retval\n",
    "\n",
    "\n",
    "def getWordSkipNames(s):\n",
    "    '''\n",
    "    find the skip word patterns\n",
    "\n",
    "    Args:\n",
    "        s(_sre.SRE_Pattern): compiled regex to match a logline\n",
    "\n",
    "    Returns:\n",
    "        retval(list(string)): list of the skip patterns found in s\n",
    "    '''\n",
    "\n",
    "    pattern = r'\\(\\(\\?\\:\\\\\\ \\{0,1\\}\\\\S\\+\\)\\{(\\d)\\,(\\d)\\}\\)'\n",
    "    matchObj = re.finditer(pattern, s.pattern, re.M | re.I)\n",
    "\n",
    "    retVal = list()\n",
    "\n",
    "    if matchObj:\n",
    "        for m in matchObj:\n",
    "            vals = m.groups()\n",
    "            fpattern = r'((?:\\ {0,1}\\S+){%i,%i})' % (int(vals[0]), int(vals[1]))\n",
    "            retVal.append(fpattern)\n",
    "\n",
    "    return retVal\n",
    "\n",
    "\n",
    "def readTemplates(sc, templateFile):\n",
    "    '''\n",
    "    returns a list of regex for replacement processing\n",
    "\n",
    "    Args:\n",
    "        sc(sparkContext): spark context\n",
    "        templateFile(string): uri to the transform file in HDFS\n",
    "\n",
    "    Returns:\n",
    "        retval(list(string))\n",
    "    '''\n",
    "\n",
    "    # map the templateFile\n",
    "    templates = sc.textFile(templateFile)\n",
    "\n",
    "    templateRDD = templates.collect()\n",
    "\n",
    "    matches = list()\n",
    "\n",
    "    for t in templateRDD:\n",
    "        stripped = r''+t.strip().rstrip()\n",
    "        escaped = re.escape(stripped)\n",
    "        replaced = unescapeSkips(escaped)\n",
    "        matches.append(replaced)\n",
    "\n",
    "    matches = rankMatches(matches)\n",
    "\n",
    "    templateLines = list()\n",
    "    for index, m in enumerate(matches):\n",
    "        # match end of line too\n",
    "        t = TemplateLine(index,\n",
    "                         re.compile(m + '$'),\n",
    "                         getWordSkipNames(re.compile(m)))\n",
    "        templateLines.append(t)\n",
    "\n",
    "    return templateLines\n",
    "\n",
    "\n",
    "def unescapeSkips(s):\n",
    "    '''\n",
    "    find an escaped version of skip{m,n} words\n",
    "    replace with unescaped version\n",
    "\n",
    "    Args:\n",
    "        s(string): string to search\n",
    "\n",
    "    Returns:\n",
    "        retval(string): string with replacement\n",
    "    '''\n",
    "\n",
    "    pattern = r'\\\\\\(\\\\\\:\\\\\\?\\\\\\ S\\\\\\+\\\\\\)\\\\\\{(\\d)\\\\\\,(\\d)\\\\\\}'\n",
    "    \n",
    "    matchObj = re.finditer(pattern, s, re.M | re.I)\n",
    "    b = s\n",
    "\n",
    "    if matchObj:\n",
    "        for m in matchObj:\n",
    "\n",
    "            newString = r'((?:\\ {0,1}\\S+){%i,%i})' % (int(m.groups()[0]),\n",
    "                                                      int(m.groups()[1]))\n",
    "\n",
    "            # the r is very important\n",
    "            newFound = r'\\\\\\(\\\\:\\\\\\?\\\\ S\\\\\\+\\\\\\)\\\\\\{%i\\\\,%i\\\\\\}' % (int(m.groups()[0]),\n",
    "                                                                    int(m.groups()[1]))\n",
    "            b = re.sub(newFound, newString, b)\n",
    "\n",
    "        return b\n",
    "    return s\n",
    "\n",
    "\n",
    "def rdd_MatchLine(line, templates):\n",
    "    '''\n",
    "    assign a log line to a templateId or -1 if no match\n",
    "    keep track of any skip word replacements, return additional\n",
    "    informaiton in the LogLine named tuple\n",
    "\n",
    "    Args:\n",
    "        line(LogLine): logline being classified\n",
    "        templates(list(TemplateLine)): templates to attempt to match to\n",
    "                                       broadcast variable\n",
    "    Returns:\n",
    "        retval(LogLine): LogLine  with the final 3 fields filled in\n",
    "                         template - actual template used for match\n",
    "                         templateId - number of the template\n",
    "                         templateDict- dictionary of skip word replacements\n",
    "    '''\n",
    "\n",
    "    for templateLine in templates.value:\n",
    "        skipFound = templateLine.template.search(line.processed)\n",
    "        templateDict = defaultdict(list)\n",
    "\n",
    "        # TODO double check that the defaultdict is working as expected\n",
    "        if skipFound:\n",
    "            for i in range(len(templateLine.skipWords)):\n",
    "                    templateDict[templateLine.skipWords[i]].append(skipFound.groups()[i])\n",
    "\n",
    "            return LogLine(line.ts,\n",
    "                           line.msg,\n",
    "                           line.processed,\n",
    "                           line.dictionary,\n",
    "                           templateLine.template.pattern,\n",
    "                           templateLine.id,\n",
    "                           templateDict)\n",
    "\n",
    "    # could not find a template match\n",
    "    return LogLine(line.ts,\n",
    "                   line.msg,\n",
    "                   line.processed,\n",
    "                   line.dictionary,\n",
    "                   None,\n",
    "                   -1,\n",
    "                   templateDict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def matchTemplates(sc, templateFile, rddLogLine):\n",
    "\n",
    "    templates = readTemplates(sc, templateFile)\n",
    "    templateBroadcast = sc.broadcast(templates)\n",
    "    return rddLogLine.map(lambda line: rdd_MatchLine(line, templateBroadcast))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timeit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logs = 'hdfs://namenode/magichour/tbird500k'\n",
    "#logs = 'hdfs://namenode/user/dgrossman/tbird.log.10000.gz'\n",
    "trans = 'hdfs://namenode/magichour/simpleTrans'\n",
    "templates = 'hdfs://namenode/magichour/templates'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rddLogs = rdd_ReadLog(sc,logs)\n",
    "procData = rdd_preProcess(sc,trans,rddLogs)\n",
    "matched = matchTemplates(sc,templates,procData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matched.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timeit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bigStuff(line):\n",
    "    inside = line.templateDict\n",
    "    for i in inside.itervalues():\n",
    "        if len(i) > 1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "xx =  matched.filter(bigStuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xxx = xx.take(1000)\n",
    "\n",
    "for x in xxx:\n",
    "    if x.templateDict is not None and len(x.templateDict) >= 1:\n",
    "        print x\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matched.saveAsPickleFile('hdfs://namenode/magichour/matchedTemplates')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
