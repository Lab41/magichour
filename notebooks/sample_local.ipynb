{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample_local.ipynb\n",
    "\n",
    "This notebook is a short example of how the MagicHour pipeline works. The code is an adaptation of driver.py located in api/local/sample/. Each section in this notebook describes a separate step in the pipeline that we use to process log files. The functions that are used here are intermediate driver functions that use the underlying MagicHour API. (The driver functions provide logging and measurements of function execution time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from magichour.api.local.sample.steps.evalapply import evalapply_step\n",
    "from magichour.api.local.sample.steps.event import event_step\n",
    "from magichour.api.local.sample.steps.genapply import genapply_step\n",
    "from magichour.api.local.sample.steps.genwindow import genwindow_step\n",
    "from magichour.api.local.sample.steps.preprocess import preprocess_step\n",
    "from magichour.api.local.sample.steps.template import template_step\n",
    "from magichour.api.local.sample.driver import *\n",
    "\n",
    "from magichour.api.local.util.log import get_logger, log_time\n",
    "from magichour.api.local.util.pickl import read_pickle_file, write_pickle_file\n",
    "\n",
    "magichour_root = os.path.dirname(os.getcwd())\n",
    "data_dir = os.path.join(magichour_root, \"magichour\", \"api\", \"local\", \"sample\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log_file = os.path.join(data_dir, \"input\", \"tbird.log.500k\")\n",
    "transforms_file = os.path.join(data_dir, \"sample.transforms\")\n",
    "\n",
    "read_lines_args = [{}, 0, 10]\n",
    "read_lines_kwargs = {\"skip_num_chars\": 22}\n",
    "logcluster_kwargs = {\"support\": \"50\"}\n",
    "paris_kwargs = {\"r_slack\": 0.0, \"num_iterations\": 3}\n",
    "gen_window_kwargs = {\"window_size\": 60, \"tfidf_threshold\": 0.0}\n",
    "\n",
    "# only return 10000 itemsets...iterations = -1 will return all\n",
    "fp_growth_kwargs = {\"min_support\": 0.005, \"iterations\": 10000, \"tfidf_threshold\": 1.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess\n",
    "\n",
    "The preprocess step takes a log file and transforms it into an iterable of LogLine named tuples. While it is possible to do this without altering the original line via the get_lines() function in templates/templates.py, it is suggested that you write some transforms (see code below) and use the get_transformed_lines() function in order to perform normalizations like converting instances of things like machine/user names or IP addresses to standard tokens. Doing this will produce much better results in the Template step. We maintain the replacements in the LogLine named tuple in order to provide the ability to reconstruct LogLines throughout the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate transformed loglines from a log file+transform file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loglines = preprocess_step(log_file, transforms_file, *read_lines_args, **read_lines_kwargs)\n",
    "write_pickle_file(loglines, loglines_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read transformed loglines from a prepared pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loglines = read_pickle_file(loglines_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can examine and interact with the loglines output in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loglines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template\n",
    "\n",
    "The template step takes in an iterable of LogLine named tuples and produces an iterable of Template named tuples. Ideally, this is the output of the previous preprocess step, however, these functions can be run independently as long as you marshal your data into iterable LogLines. In fact, the MagicHour API was designed with this in mind since we anticipate that users will want to mix and match different pipeline modules.\n",
    "\n",
    "We provide two possible templating algorithms: LogCluster and StringMatch. Additional details about LogCluster is  available at http://ristov.github.io/logcluster/. For additional details about StringMatch, see the paper \"[One Graph Is Worth a Thousand Logs: Uncovering Hidden Structures in Massive System Event Logs](http://link.springer.com/chapter/10.1007%2F978-3-642-04180-8_32)\" by Aharon, Barash, Cohen, and Mordechai. There is also a [video available](http://videolectures.net/ecmlpkdd09_barash_gwtluhsmsel/) that provides more information about StringMatch.\n",
    "\n",
    "*Note: The name \"StringMatch\" was taken from another [paper](http://users.cis.fiu.edu/~taoli/pub/liang-cikm2011.pdf) (Aharon et al do not name their algorithm).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogCluster\n",
    "\n",
    "Generate templates using the LogCluster algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen_templates = template_step(loglines, \"logcluster\", **logcluster_kwargs)\n",
    "write_pickle_file(gen_templates, gen_templates_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **(\\*\\*WIP\\*\\*)** StringMatch\n",
    "\n",
    "Generate tempaltes using the StringMatch algorithm. StringMatch uses cosine similarity to group log lines which are alike. You should use LogCluster if you aren't tolerant of lossy templating -- though it should be noted that the preprocess step should help to mitigate the loss from StringMatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gen_templates = template_step(loglines, \"stringmatch\")\n",
    "write_pickle_file(gen_templates, gen_templates_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read templates from a prepared pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-02-17 14:40:13,916 [INFO] [magichour.api.local.util.pickl] Reading pickle file: /Users/kylez/lab41/magichour/magichour/magichour/api/local/sample/data/pickle/gen_templates.pickle\n"
     ]
    }
   ],
   "source": [
    "gen_templates = read_pickle_file(gen_templates_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can examine and interact with the templates output in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen_templates[75]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Templates\n",
    "\n",
    "The apply templates step takes in an iterable of LogLine named tuples and an iterable of Template named tuples (i.e. output of previous templating step). In this instance, we are applying the generated templates on the same log file that they came from. We believe that both this step and the Apply Events step (further down) are the only two steps that are needed to be implemented in a streaming fashion (to be able to keep up with log file ingest rates). The remainder of the steps described in this notebook could theoretically be run offline nightly (i.e. batch processing).\n",
    "\n",
    "The output of the apply templates step is an iterable of TimedTemplate named tuples, representing the instances of each template that were found in the log file. If the template_id is -1 in a TimedTemplate, then that means that no template was found that matches that particular line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create timed templates by applying templates generated from the last step (Template) over an iterable of LogLines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_loglines = genapply_step(loglines, gen_templates)\n",
    "write_pickle_file(eval_loglines, eval_loglines_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read timed templates from a prepared pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_loglines = read_pickle_file(eval_loglines_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can examine and interact with the timed_templates output in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval_loglines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window\n",
    "\n",
    "The window step takes in an iterable of TimedTemplate named tuples (i.e. the output of the apply templates step) and returns an iterable of sets containing TimedTemplate instances. Each of these windows represent a time range in which the contained template_id's co-occurred. . In effect, we are creating transactions by grouping all TimedTemplates within *window_size*. These transactions will be passed to the next step which will perform market basket analysis on them in order to identify frequently co-occurring itemsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create windows using timed templates generated from the last step (Apply Templates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_windows = genwindow_step(eval_loglines, **gen_window_kwargs)\n",
    "write_pickle_file(gen_windows, gen_windows_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read windows from a prepared pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-02-17 14:39:07,750 [INFO] [magichour.api.local.util.pickl] Reading pickle file: /Users/kylez/lab41/magichour/magichour/magichour/api/local/sample/data/pickle/gen_windows.pickle\n"
     ]
    }
   ],
   "source": [
    "gen_windows = read_pickle_file(gen_windows_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can examine and interact with the window output in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen_windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(gen_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fp_growth\n",
    "\n",
    "Generate events by applying the fp_growth algorithm on windows created from last step (Window)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen_events = event_step(gen_windows, \"fp_growth\", **fp_growth_kwargs)\n",
    "write_pickle_file(gen_events, gen_events_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARIS\n",
    "\n",
    "Generate events by applying the PARIS algorithm on windows created from last step (Window)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-02-17 14:43:54,070 [INFO] [magichour.api.local.sample.steps.event] Running PARIS algorithm... ({'r_slack': 0.0, 'num_iterations': 3})\n",
      "2016-02-17 14:43:54,071 [INFO] [magichour.api.local.sample.steps.event] {'r_slack': 0.0, 'num_iterations': 3}\n",
      "2016-02-17 14:44:56,604 [INFO] [magichour.api.local.sample.steps.event] ==========Custom post processing for sample data==========\n",
      "2016-02-17 14:44:56,605 [INFO] [magichour.api.local.sample.steps.event] Applying a tfidf filter to each event's template_ids. (threshold = 1.0)\n",
      "2016-02-17 14:44:56,606 [INFO] [magichour.api.local.util.modelgen] Removing subsets from tfidf_filter result...\n",
      "2016-02-17 14:44:56,607 [INFO] [magichour.api.local.sample.steps.event] ==========End custom post processing==========\n",
      "2016-02-17 14:44:56,608 [INFO] [magichour.api.local.util.pickl] Writing data to pickle file: /Users/kylez/lab41/magichour/magichour/magichour/api/local/sample/data/pickle/gen_events.pickle\n"
     ]
    }
   ],
   "source": [
    "gen_events = event_step(gen_windows, \"paris\", **paris_kwargs)\n",
    "write_pickle_file(gen_events, gen_events_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read events from a prepared pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gen_events = read_pickle_file(gen_events_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can examine and interact with the event output in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Event(id='81f6d8d4-72bd-4ced-a979-9e9f63553b07', template_ids=frozenset([195, 388, 327, 177, 396, 18, 334, 400, 17, 370, 20, 270, 152, 58])),\n",
       " Event(id='181794fd-a6ca-478c-bf32-f42e7e628b45', template_ids=frozenset([480, 481, 482, 483, 484, 168, 105, 146, 213, 316, 508, 189])),\n",
       " Event(id='03064351-be3b-4cf8-9adc-85037502f777', template_ids=frozenset([97, 194, 4, 5, 38, 71, 296, 388, 503, 430, 527, 336, 369, 431, 277, 150, 495, 217, 27, 28, 230])),\n",
       " Event(id='7caae362-f2fb-4ec2-9302-afe2b38513a5', template_ids=frozenset([571, 548, 326, 582, 295, 488, 9, 267, 399, 210, 563, 564, 411, 126])),\n",
       " Event(id='6c1ac53f-5e99-4e23-9ab1-ac98b7b193d5', template_ids=frozenset([6, 7, 8, 265, 10, 11, 12, 13, 14, 15, 145, 274, 275, 276, 410, 32, 294, 171, 44, 562, 568, 266, 578, 581, 584, 588, 462, 337, 210, 349, 229, 573, 378, 252, 253, 511])),\n",
       " Event(id='3873e5b8-9829-4ec6-bb9b-0a67538bcc46', template_ids=frozenset([128, 129, 130, 131, 360, 29, 219, 317])),\n",
       " Event(id='01836373-62ff-4fd6-b1a2-2c57e4033495', template_ids=frozenset([85, 167])),\n",
       " Event(id='e08cc191-02d9-49e8-aa82-e0713ea3dece', template_ids=frozenset([67, 425, 205, 47, 451, 372, 222, 86, 409, 380, 446, 351])),\n",
       " Event(id='f134c03d-84fe-4283-8a9b-43b2ec30147d', template_ids=frozenset([104, 158, 22, 367])),\n",
       " Event(id='a94eec2e-c0c4-4525-8823-28b3b4f409b0', template_ids=frozenset([177, 162, 396, 270, 334])),\n",
       " Event(id='7c9dbcc1-96b4-45ba-acb1-d81a054b6473', template_ids=frozenset([480, 1, 325, 584, 335, 276, 213, 374, 316, 229]))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the templates which comprise each event that we identified in this step. In this example we've found what looks to be:\n",
    "\n",
    "* a DHCP request\n",
    "* some type of change in the network\n",
    "* an SSH login\n",
    "\n",
    "The parameters that we chose, along with the TF-IDF filtering done in the windowing and event discovery steps, made the discovery pretty restrictive. Playing around with the settings and removing the filtering will cause the pipeline to discover other event types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['195: USERINT AFILE[INT]: [INFO]: Generate SM IN_SERVICE trap for KEYVALUE',\n",
      "  '388: #INT# logger: Kickstart Install: SISUITE Client RPMS',\n",
      "  '327: #INT# logger: Kickstart Install: setup CAP sysconfig file',\n",
      "  '177: USERINT AFILE[INT]: [INFO]: Configuration caused by discovering new ports',\n",
      "  '396: USERINT AFILE[INT]: [FILEANDLINE]: Topology changed',\n",
      "  '18: USERINT AFILE[INT]: [FILEANDLINE]: Force neighbor port (KEYVALUE, KEYVALUE, KEYVALUE) to DOWN because (INT) INTst sweep or (INT) role change.',\n",
      "  '334: USERINT AFILE[INT]: [FILEANDLINE]: Rediscover the subnet',\n",
      "  '400: #INT# logger: Kickstart Install: OSCAR modules RPMS',\n",
      "  '17: USERINT AFILE[INT]: [FILEANDLINE]: Program port state, KEYVALUE, KEYVALUEINT, current state INT, neighbor KEYVALUE, KEYVALUEINT, current state INT',\n",
      "  '370: #INT# logger: Kickstart Install: SNL COE Legal Banner',\n",
      "  '20: USERINT AFILE[INT]: [FILEANDLINE]: Force port (KEYVALUE, KEYVALUE, KEYVALUE) to DOWN because (INT) INTst sweep or (INT) role change.',\n",
      "  '270: USERINT AFILE[INT]: [FILEANDLINE]: async events require sweep',\n",
      "  '152: USERINT AFILE[INT]: [INFO]: Configuration caused by multicast membership change',\n",
      "  '58: USERINT AFILE[INT]: [FILEANDLINE]: A new IB node INTxMEMADDR was discovered and assigned LID INT'],\n",
      " ['480: #INT# sshd[INT]: connection from \"#INT#\"',\n",
      "  '481: USERINT crond[INT]: (USER) CMD FILEPATH d)',\n",
      "  '482: USERINT crond[INT]: (USER) CMD FILEPATH e)',\n",
      "  '483: USERINT crond[INT]: (USER) CMD FILEPATH b)',\n",
      "  '484: USERINT crond[INT]: (USER) CMD FILEPATH c)',\n",
      "  '168: USERINT crond(pam_unix)[INT]: session opened for user USER by (KEYVALUE)',\n",
      "  '105: USERINT sendmail[INT]: My unqualified host name (USERINT) unknown; sleeping for retry',\n",
      "  '146: USER sendmail[INT]: My unqualified host name (USER) unknown; sleeping for retry',\n",
      "  '213: #INT# sshd[INT]: User #INT#, coming from #INT#, authenticated.',\n",
      "  \"316: #INT# sshdINT[INT]: Now running on #INT#'s privileges.\",\n",
      "  '508: USER crond[INT]: (USER) CMD FILEPATH a)',\n",
      "  '189: USER crond(pam_unix)[INT]: session opened for user USER by (KEYVALUE)'],\n",
      " ['97: USER sendmail[INT]: unable to qualify my own domain name (USER) -- using short name',\n",
      "  '194: USER sshd(pam_unix)[INT]: session opened for user USER by (KEYVALUE)',\n",
      "  '4: USERINT sendmail[INT]: *{1,1} KEYVALUE, KEYVALUE FILEPATH KEYVALUE:INT:INT, KEYVALUE:INT:INT, KEYVALUE, KEYVALUE, KEYVALUEIPADDR] [IPADDR], KEYVALUE, KEYVALUE: Connection refused by [IPADDR]',\n",
      "  '5: USER sendmail[INT]: *{1,1} KEYVALUE, KEYVALUE FILEPATH KEYVALUE:INT:INT, KEYVALUE:INT:INT, KEYVALUE, KEYVALUE, KEYVALUEIPADDR] [IPADDR], KEYVALUE, KEYVALUE: Connection refused by [IPADDR]',\n",
      "  \"38: #INT# userhelper[INT]: running FILEPATH --nox -i ganglia-gmond' with USER privileges on behalf of 'USER'\",\n",
      "  '71: USERINT sendmail[INT]: unable to qualify my own domain name (USERINT) -- using short name',\n",
      "  '296: USERINT crond(pam_unix)[INT]: session closed for user USER',\n",
      "  '388: #INT# logger: Kickstart Install: SISUITE Client RPMS',\n",
      "  '503: #INT# logger: Kickstart: configure services',\n",
      "  '430: #INT# logger: Kickstart Install: SUN JDK Package',\n",
      "  '527: #INT# logger: Kickstart: setup firstboot',\n",
      "  '336: USER crond(pam_unix)[INT]: session closed for user USER',\n",
      "  '369: #INT# logger: Kickstart Install: pdsh + ssh packages',\n",
      "  '431: USER xinetd[INT]: START: rsync KEYVALUE KEYVALUE',\n",
      "  '277: #INT# logger: Kickstart Install: Prepare speconf_sync to work',\n",
      "  '150: USER sshd[INT]: Accepted publickey for USER from ::ffff:IPADDR port INT sshINT',\n",
      "  '495: #INT# logger: Kickstart Install: oneSIS RPM',\n",
      "  '217: #INT# logger: Kickstart: setup firstboot for ganglia client software',\n",
      "  '27: USERINT sendmail[INT]: *{1,1} KEYVALUE, KEYVALUE, KEYVALUE, KEYVALUE, KEYVALUEAFILE@USERINT>, KEYVALUEINT#@localhost',\n",
      "  '28: USER sendmail[INT]: *{1,1} KEYVALUE, KEYVALUE, KEYVALUE, KEYVALUE, KEYVALUEAFILE@USER>, KEYVALUEINT#@localhost',\n",
      "  '230: #INT# logger: Kickstart Install: Torque Mom and Maui speconf Tree'],\n",
      " ['571: #INT# syslog: klogd shutdown failed',\n",
      "  '548: #INT# syslog: syslogd startup succeeded',\n",
      "  '326: #INT# logger: Kickstart: setup soft links and NFS mounts',\n",
      "  '582: #INT# syslogd AFILE: restart.',\n",
      "  '295: #INT# kernel: klogd AFILE, log source = FILEPATH started.',\n",
      "  '488: #INT# logger: Kickstart Install: CAP tarball',\n",
      "  '9: #INT# kernel: audit(FILEANDLINE): avc: denied { write } for KEYVALUE KEYVALUE KEYVALUE KEYVALUE KEYVALUE KEYVALUE:system_r:syslogd_t KEYVALUE:object_r:file_t KEYVALUE',\n",
      "  '267: #INT# logger: Kickstart Install: Reducing mingetty count ...',\n",
      "  '399: #INT# logger: Kickstart: Configure sysctl parameters',\n",
      "  '210: #INT# userhelper[INT]: pam_timestamp: updated timestamp file FILEPATH',\n",
      "  '563: #INT# syslog: klogd startup succeeded',\n",
      "  '564: #INT# logger: Install: apply updates',\n",
      "  '411: #INT# logger: Kickstart: setup authorization schema',\n",
      "  \"126: #INT# userhelper[INT]: running FILEPATH with USER privileges on behalf of 'USER'\"],\n",
      " ['6: #INT# kernel: audit(FILEANDLINE): avc: denied { read write } for KEYVALUE KEYVALUE KEYVALUE:[INT] KEYVALUE KEYVALUE KEYVALUE:system_r:ntpd_t KEYVALUE:system_r:unconfined_t KEYVALUE',\n",
      "  '7: #INT# kernel: audit(FILEANDLINE): avc: denied { read } for KEYVALUE KEYVALUE FILEPATH (deleted) KEYVALUE KEYVALUE KEYVALUE:system_r:ntpd_t KEYVALUE:object_r:ramfs_t KEYVALUE',\n",
      "  '8: #INT# kernel: audit(FILEANDLINE): avc: denied { read write } for KEYVALUE KEYVALUE KEYVALUE KEYVALUE KEYVALUE KEYVALUE:system_r:ntpd_t KEYVALUE:object_r:file_t KEYVALUE',\n",
      "  '265: #INT# ntpd[INT]: frequency initialized AFILE PPM from FILEPATH',\n",
      "  '10: #INT# kernel: audit(FILEANDLINE): avc: denied { append } for KEYVALUE KEYVALUE FILEPATH KEYVALUE KEYVALUE KEYVALUE:system_r:ntpd_t KEYVALUE:object_r:ramfs_t KEYVALUE',\n",
      "  '11: #INT# kernel: audit(FILEANDLINE): avc: denied { getattr } for KEYVALUE KEYVALUE FILEPATH KEYVALUE KEYVALUE KEYVALUE:system_r:ntpd_t KEYVALUE:object_r:file_t KEYVALUE',\n",
      "  '12: #INT# kernel: audit(FILEANDLINE): avc: denied { search } for KEYVALUE KEYVALUE KEYVALUE KEYVALUE KEYVALUE KEYVALUE:system_r:ntpd_t KEYVALUE:object_r:file_t KEYVALUE',\n",
      "  '13: #INT# kernel: audit(FILEANDLINE): avc: denied { read } for KEYVALUE KEYVALUE FILEPATH KEYVALUE KEYVALUE KEYVALUE:system_r:ntpd_t KEYVALUE:object_r:ramfs_t KEYVALUE',\n",
      "  '14: #INT# kernel: audit(FILEANDLINE): avc: denied { write } for KEYVALUE KEYVALUE KEYVALUE KEYVALUE KEYVALUE KEYVALUE:system_r:ntpd_t KEYVALUE:object_r:file_t KEYVALUE',\n",
      "  '15: #INT# kernel: audit(FILEANDLINE): avc: denied { read } for KEYVALUE KEYVALUE FILEPATH KEYVALUE KEYVALUE KEYVALUE:system_r:ntpd_t KEYVALUE:object_r:file_t KEYVALUE',\n",
      "  '145: #INT# ntpd[INT]: ntpd #INT#@AFILE-r Mon Oct INT INT:INT:INT EDT INT (INT)',\n",
      "  '274: #INT# ntpd[INT]: Listening on interface wildcard, IPADDR#INT',\n",
      "  '275: #INT# logger: Kickstart: setup the cisco (topspin) ib stack',\n",
      "  '276: #INT# logger: FILEPATH Install: Finished installation on AFILE',\n",
      "  '410: #INT# logger: Kickstart Install: Dell OMSA Package',\n",
      "  \"32: #INT# userhelper[INT]: running FILEPATH --nox -i -f kernel-devel' with USER privileges on behalf of 'USER'\",\n",
      "  '294: #INT# ntpd[INT]: Listening on interface ethINT, IPADDR#INT',\n",
      "  '171: #INT# ntpd[INT]: configure: keyword \"authenticate\" unknown, line ignored',\n",
      "  \"44: #INT# userhelper[INT]: running FILEPATH --nox -i netsnmp' with USER privileges on behalf of 'USER'\",\n",
      "  '562: #INT# gmond: gmond shutdown succeeded',\n",
      "  '568: #INT# gmond: gmond startup succeeded',\n",
      "  '266: #INT# logger: Kickstart: setup firstboot for tbird panasas env',\n",
      "  '578: #INT# ntpd: ntpd shutdown failed',\n",
      "  '581: #INT# logger: setup NTP client',\n",
      "  '584: #INT# exiting on signal INT',\n",
      "  '588: #INT# ntpd: succeeded',\n",
      "  '462: #INT# ntpd[INT]: kernel time sync status INT',\n",
      "  '337: #INT# ntpd[INT]: Listening on interface lo, IPADDR#INT',\n",
      "  '210: #INT# userhelper[INT]: pam_timestamp: updated timestamp file FILEPATH',\n",
      "  '349: #INT# logger: Kickstart Install: Dell BIOS CONF Package',\n",
      "  '229: #INT# logger: Kickstart Install: TBIRD Dell FIRMWARE Update Module',\n",
      "  '573: #INT# ntpd: ntpd startup succeeded',\n",
      "  '378: #INT# logger: Kickstart Install: tbird kernel package',\n",
      "  '252: #INT# logger: Kickstart Install: Setup to become a Tbird login',\n",
      "  '253: #INT# ntpdate[INT]: step time server IPADDR offset -AFILE sec',\n",
      "  '511: #INT# ntpd[INT]: precision = AFILE usec'],\n",
      " ['128: USER FILEPATH data_thread() got not answer from any [Thunderbird_BINT] datasource',\n",
      "  '129: USER FILEPATH data_thread() got not answer from any [Thunderbird_AINT] datasource',\n",
      "  '130: USER FILEPATH data_thread() got not answer from any [Thunderbird_CINT] datasource',\n",
      "  '131: USER FILEPATH data_thread() got not answer from any [Thunderbird_DINT] datasource',\n",
      "  '360: USERINT AFILE[INT]: [FILEANDLINE]: No topology change',\n",
      "  '29: USERINT AFILE[INT]: [FILEANDLINE]: ********************** NEW SWEEP ********************',\n",
      "  '219: USERINT AFILE[INT]: [FILEANDLINE]: No configuration change required',\n",
      "  '317: MACHINENAME ntpd[INT]: synchronized to IPADDR, stratum INT'],\n",
      " ['85: USERINT AFILE[INT]: [FILEANDLINE]: An existing IB node GUID MEMADDR LID INT was removed',\n",
      "  '167: USERINT AFILE[INT]: [INFO]: Generate SM OUT_OF_SERVICE trap for KEYVALUE'],\n",
      " ['67: USERINT dhcpd: DHCPREQUEST for IPADDR (IPADDR) from MACADDR via ethINT: unknown lease IPADDR.',\n",
      "  '425: USERINT dhcpd: DHCPDISCOVER from MACADDR via ethINT',\n",
      "  '205: USERINT dhcpd: DHCPREQUEST for IPADDR (IPADDR) from MACADDR via ethINT',\n",
      "  '47: aadmMACHINENAME dhcpd: DHCPREQUEST for IPADDR (IPADDR) from MACADDR via ethINT: unknown lease IPADDR.',\n",
      "  '451: USER dhcpd: DHCPDISCOVER from MACADDR via ethINT',\n",
      "  '372: USERINT dhcpd: DHCPACK on IPADDR to MACADDR via ethINT',\n",
      "  '222: USER dhcpd: DHCPREQUEST for IPADDR (IPADDR) from MACADDR via ethINT',\n",
      "  '86: aadmMACHINENAME dhcpd: DHCPDISCOVER from MACADDR via ethINT: network A_net: no free leases',\n",
      "  '409: USER dhcpd: DHCPACK on IPADDR to MACADDR via ethINT',\n",
      "  '380: USER dhcpd: DHCPOFFER on IPADDR to MACADDR via ethINT',\n",
      "  '446: USER xinetd[INT]: START: tftp KEYVALUE KEYVALUE',\n",
      "  '351: USERINT dhcpd: DHCPOFFER on IPADDR to MACADDR via ethINT'],\n",
      " ['104: MACHINENAME sshd[INT]: Accepted publickey for USER from ::ffff:IPADDR port INT sshINT',\n",
      "  '158: MACHINENAME sshd(pam_unix)[INT]: session opened for user USER by (KEYVALUE)',\n",
      "  '22: USER FILEPATH RRD_update FILEPATH FILEPATH illegal attempt to update using time INT when last update time is INT (minimum one second step)',\n",
      "  '367: USERINT ntpd[INT]: synchronized to IPADDR, stratum INT'],\n",
      " ['177: USERINT AFILE[INT]: [INFO]: Configuration caused by discovering new ports',\n",
      "  '162: USERINT AFILE[INT]: [INFO]: Configuration caused by discovering removed ports',\n",
      "  '396: USERINT AFILE[INT]: [FILEANDLINE]: Topology changed',\n",
      "  '270: USERINT AFILE[INT]: [FILEANDLINE]: async events require sweep',\n",
      "  '334: USERINT AFILE[INT]: [FILEANDLINE]: Rediscover the subnet'],\n",
      " ['480: #INT# sshd[INT]: connection from \"#INT#\"',\n",
      "  '1: MACHINENAME Server Administrator: Instrumentation Service EventID: INT Temperature sensor returned to a normal value Sensor location: BMC Ambient Temp Chassis location: Main System Chassis Previous state was: Non-Critical (Warning) Temperature sensor value (in Degrees Celsius): AFILE',\n",
      "  \"325: #INT# sshd[INT]: connection lost: 'Connection closed.'\",\n",
      "  '584: #INT# exiting on signal INT',\n",
      "  '335: #INT# sshd[INT]: Local disconnected: Connection closed.',\n",
      "  '276: #INT# logger: FILEPATH Install: Finished installation on AFILE',\n",
      "  '213: #INT# sshd[INT]: User #INT#, coming from #INT#, authenticated.',\n",
      "  '374: MACHINENAME snmpd[INT]: Got trap from peer on fd INT',\n",
      "  \"316: #INT# sshdINT[INT]: Now running on #INT#'s privileges.\",\n",
      "  '229: #INT# logger: Kickstart Install: TBIRD Dell FIRMWARE Update Module']]\n"
     ]
    }
   ],
   "source": [
    "template_d = {template_id : template for (template_id, template) in [(template.id, template) for template in gen_templates]}\n",
    "e = []\n",
    "for event in gen_events:\n",
    "    ts = []\n",
    "    for template_id in event.template_ids:\n",
    "        ts.append(\"%s: %s\" % (template_id, template_d[template_id].raw_str))\n",
    "    e.append(ts)\n",
    "from pprint import pprint\n",
    "pprint(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create timed templates by applying templates generated from the last step (Template) over an iterable of LogLines.\n",
    "\n",
    "The way this currently works is by determining which template for each event is the least frequently occurring template in timed_templates. By using the least frequently occuring template, we are guaranteed to not discover more than the maximum number of events allowed in a given list of timed_templates. \n",
    "\n",
    "For each event's least frequently occurring template, we construct a \"window\" around each instance in timed_templates, where a \"window\" is a collection of all of the timed_templates that occurred within the specified window size (60 secs by default). Within each window, for each of the remaining template types belonging to the event in question, we say that the logline with the highest Jaccard similarity score between its replacement values and the original frequently occuring template's replacement values. The logic here is that the closer the Jaccard similarity score, the more likely that the two loglines are talking about the same (machine, IP address, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timed_events = evalapply_step(gen_events, eval_loglines, loglines)\n",
    "\n",
    "#write_pickle_file(timed_events, timed_events_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read timed events from a prepared pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timed_events = read_pickle_file(timed_events_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can examine and interact with the window output in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "timed_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(timed_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[te.event_id for te in timed_events]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
