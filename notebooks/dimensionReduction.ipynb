{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from collections import namedtuple\n",
    "import datetime\n",
    "import hashlib\n",
    "import sys\n",
    "import time\n",
    "import signal\n",
    "import re\n",
    "import gzip\n",
    "%matplotlib\n",
    "LogLine = namedtuple('LogLine', ['ts', 'text', 'processed'])\n",
    "DataRecord = namedtuple('DataRecord', ['line', 'md5hash', 'stats', 'processedStats'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def openFile(name, mode):\n",
    "    if name.tolower().endswith('.gz'):\n",
    "        return gzip.open(name, mode+'b')\n",
    "    else:\n",
    "        return open(name, mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tuple2Str(a):\n",
    "    '''\n",
    "         make a concatenation of a tuple\n",
    "         can make multiple things alias to the same comparison..\n",
    "         'a','aaa','aa','aa','aaa','a' all map to 'aaaa'\n",
    "    '''\n",
    "\n",
    "    return '%s%s' % a\n",
    "\n",
    "\n",
    "# GOOD\n",
    "def str2Super(X):\n",
    "    '''\n",
    "        make super words\n",
    "    '''\n",
    "    return sorted(map(tuple2Str, set(combinations(X.rstrip().split(), 2))))\n",
    "\n",
    "\n",
    "def processString(inText):\n",
    "    FLAGS = re.MULTILINE | re.DOTALL\n",
    "    URL = ' URL '\n",
    "    FILEPATH = ' FILEPATH '\n",
    "    IPADDR = ' IPADDR '\n",
    "    FILEANDLINE = ' FILEANDLINE '\n",
    "    DATE = ' DATE '\n",
    "    TIME = ' TIME '\n",
    "    SILENTREMOVE = ''\n",
    "    SPACE = ' '\n",
    "    PERL = ' PERLFILE '\n",
    "    CGI = ' CGIFILE '\n",
    "    JPG = ' JPGFILE '\n",
    "    AFILE = ' AFILE '\n",
    "    LEVEL = ' LEVEL '\n",
    "    INT = ' INT '\n",
    "    \n",
    "    badchars = [r'\\[',r'\\]',r'\\(',r'\\)',r'{',r'}',r':',r',',r'-']\n",
    "    silentchars = [r'\\\"',r'\\.',r'\\'',r'\\`',r'!']\n",
    "    text = \"\"+inText.lower()\n",
    "\n",
    "    #text = re.sub(r\"https?:\\/\\/\\S+\\b|www\\.(\\w+\\.)+\\S*\", URL,inText,FLAGS)\n",
    "    #2010-04-01 00:39:21,914\n",
    "\n",
    "    \n",
    "    text = re.sub(r'(?:\\d{2}:\\d{2}:\\d{2},\\d{3})',TIME,text,FLAGS)\n",
    "    text = re.sub(r'(?:\\d{4}-\\d{2}-\\d{2})',DATE,text,FLAGS)\n",
    "    text = re.sub(r'(?:\\w+(\\.?)+:\\d+)',FILEANDLINE,text,FLAGS) #thisone bad\n",
    "    text = re.sub(r'https?:\\/\\/\\S+',URL,text,FLAGS)\n",
    "    #text = re.sub(r'(?:\\w+\\.cgi)',CGI,text,FLAGS)\n",
    "    #text = re.sub(r'(?:\\w+\\.jpg)',JPG,text,FLAGS)\n",
    "    text = re.sub(r'(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)',IPADDR,text,FLAGS)\n",
    "    text = re.sub(r'(\\S+)\\/([^\\/]?)(?:\\S+)',FILEPATH,text,FLAGS)\n",
    "    text = re.sub(r'(?:(\\w+\\.)+\\w{1,3})',AFILE,text,FLAGS)\n",
    "    text = re.sub(r'alert|error|crit',LEVEL,text,FLAGS)\n",
    "\n",
    "    text = re.sub(r'(?:\\d+)',INT,text,FLAGS)\n",
    "    \n",
    "    for c in badchars:\n",
    "        text = re.sub(c,SPACE,text,FLAGS)\n",
    "        \n",
    "    for c in silentchars:\n",
    "            text = re.sub(c,SILENTREMOVE,text,FLAGS)\n",
    "\n",
    "    text = re.sub(r'\\s+',' ',text,FLAGS)\n",
    "    \n",
    "    print inText\n",
    "    print text\n",
    "    \n",
    "    return text.lstrip().rstrip()\n",
    "    \n",
    "\n",
    "\n",
    "def dataset_iterator(fIn, num_lines):\n",
    "    '''\n",
    "        Handle reading the data from file into a know form\n",
    "    '''\n",
    "    lines_read = 0\n",
    "    success_full = 0\n",
    "    while num_lines == -1 or lines_read < num_lines:\n",
    "        lines_read += 1\n",
    "        line = fIn.readline()\n",
    "        if len(line) == 0:\n",
    "            break\n",
    "        else:\n",
    "            processed = processString(line[27:].strip())\n",
    "            try:\n",
    "                \n",
    "                logtype = 1\n",
    "                if logtype == 0:\n",
    "                    # syslogway\n",
    "                    ts = datetime.datetime.strptime(line[:14], '%b %d %H:%M:%S')\n",
    "                    rest = line[15:].strip()\n",
    "                    yield LogLine(ts.replace(year=2015), rest,processed)\n",
    "                    success_full += 1\n",
    "                if logtype == 1:\n",
    "                    # apache weblog way\n",
    "                    ts = datetime.datetime.strptime(line[1:25],\n",
    "                                                    '%a %b %d %H:%M:%S %Y')\n",
    "                    rest = line[27:].strip()\n",
    "                    processed = processString(rest)\n",
    "                    yield LogLine(ts, rest, processed)\n",
    "                    success_full += 1\n",
    "                    \n",
    "                print processed\n",
    "            except:\n",
    "                pass\n",
    " \n",
    "# TODO lookup faster hashes\n",
    "def makeHash(s):\n",
    "    '''\n",
    "        make a md5 string rep of an input string\n",
    "    '''\n",
    "\n",
    "    m = hashlib.md5()\n",
    "    m.update(s)\n",
    "    return m.hexdigest()\n",
    "\n",
    "\n",
    "\n",
    "def init(inFile):\n",
    "\n",
    "    totalS = time.time()\n",
    "\n",
    "    print 'Attempting to open %s' % ( inFile)\n",
    "    out = sys.stdout\n",
    "\n",
    "    a = openFile(inFile, 'r')\n",
    "    D = list()\n",
    "    G = dict()\n",
    "\n",
    "    readCount = 0\n",
    "    \n",
    "    for r in dataset_iterator(a, -1):\n",
    "        h = makeHash(r.text)\n",
    "        s = str2Super(r.text)\n",
    "        ps = str2Super(r.processed)\n",
    "        D.append(DataRecord(r, h, s,ps))\n",
    "        readCount += 1\n",
    "\n",
    "    a.close()\n",
    "\n",
    "    print 'Read %i items' % readCount\n",
    "    \n",
    "    return D\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D = init('apache_error_1K.log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allTuples = [str.join(\" \",X.processedStats) for X in D]\n",
    "allOriginal = [X.line.text for X in D]\n",
    "allProcessed =[X.line.processed for X in D]\n",
    "print 'original:',allOriginal[0]\n",
    "print 'processed:',allProcessed[0]\n",
    "print 'tupleified:',allTuples[0]\n",
    "\n",
    "firstAndSecondOrders = list()\n",
    "for i in range(len(allTuples)):\n",
    "    firstAndSecondOrders.append(allTuples[i] + ' ' +allProcessed[i] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genVectorsFromThis = firstAndSecondOrders\n",
    "s=set()\n",
    "x=0\n",
    "for i in range(len(genVectorsFromThis)):\n",
    "    s.add(genVectorsFromThis[i])\n",
    "    x+=1\n",
    "    if len(s) == 100:\n",
    "        break\n",
    "print x,len(s)\n",
    "for i in s:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2, max_features = 1000)\n",
    "vz = vectorizer.fit_transform(genVectorsFromThis)\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=80, random_state=0)\n",
    "svd_tfidf = svd.fit_transform(vz[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svd_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0)\n",
    "tsne_tfidf = tsne_model.fit_transform(svd_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsne_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsne_tfidf[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "\n",
    "output_notebook()\n",
    "plot_tfidf = bp.figure(plot_width=1024, plot_height=1024, title=\"LogLines (tf-idf)\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "    x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "plot_tfidf.scatter(x=tsne_tfidf[:,0], y=tsne_tfidf[:,1],\n",
    "                    source=bp.ColumnDataSource({\n",
    "                        \"LogLine\": allOriginal[:1000],\n",
    "                        \"LineProcessed\":genVectorsFromThis[:1000]\n",
    "                    }))\n",
    "\n",
    "hover = plot_tfidf.select(dict(type=HoverTool))\n",
    "hover.tooltips={\"LogLine\": \"@LogLine \\n(PROCESSED: \\\"@LineProcessed\\\")\"}\n",
    "show(plot_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "num_clusters=20\n",
    "kmeans_model = MiniBatchKMeans(n_clusters=num_clusters, init='k-means++', n_init=1, \n",
    "                         init_size=1000, batch_size=1000, verbose=False, max_iter=1000)\n",
    "kmeans = kmeans_model.fit(vz)\n",
    "kmeans_clusters = kmeans.predict(vz)\n",
    "kmeans_distances = kmeans.transform(vz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i,line in enumerate(genVectorsFromThis):\n",
    "    if (i<5):\n",
    "        print(\"Cluster \"+ str(kmeans_clusters[i]) + \":\" + line + \"(distance: \" + str(kmeans_distances[i][kmeans_clusters[i]]) + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(num_clusters):\n",
    "    sys.stdout.write(\"Cluster %d:\" % i)\n",
    "    for j in sorted_centroids[i, :10]:\n",
    "        sys.stdout.write (' %s' % terms[j])\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsne_kmeans = tsne_model.fit_transform(kmeans_distances[:10000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "colormap = np.array([\n",
    "    \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\", \n",
    "    \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#c5b0d5\", \n",
    "    \"#8c564b\", \"#c49c94\", \"#e377c2\", \"#f7b6d2\", \"#7f7f7f\", \n",
    "    \"#c7c7c7\", \"#bcbd22\", \"#dbdb8d\", \"#17becf\", \"#9edae5\"\n",
    "])\n",
    "\n",
    "plot_kmeans = bp.figure(plot_width=1024, plot_height=1024, title=\"tuple k-means\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "    x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "plot_kmeans.scatter(x=tsne_kmeans[:,0], y=tsne_kmeans[:,1], \n",
    "                    color=colormap[kmeans_clusters][:10000], \n",
    "                    source=bp.ColumnDataSource({\n",
    "                         \"LogLine\": allOriginal[:1000],\n",
    "                         \"LineProcessed\":genVectorsFromThis[:1000],\n",
    "                         \"cluster\": kmeans_clusters[:10000]\n",
    "                    }))\n",
    "hover = plot_kmeans.select(dict(type=HoverTool))\n",
    "hover.tooltips={\"LogLine\": \"@LogLine \\n(PROCESSED: \\\"@LineProcessed\\\")\"}\n",
    "show(plot_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lda\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cvectorizer = CountVectorizer(min_df=4, max_features=10000, stop_words='english')\n",
    "cvz = cvectorizer.fit_transform(genVectorsFromThis)\n",
    "\n",
    "n_topics = 20\n",
    "n_iter = 1000\n",
    "lda_model = lda.LDA(n_topics=n_topics, n_iter=n_iter)\n",
    "X_topics = lda_model.fit_transform(cvz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "\n",
    "topic_word = lda_model.topic_word_  # get the topic words\n",
    "vocab = cvectorizer.get_feature_names()\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tsne_lda = tsne_model.fit_transform(X_topics[:10000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_topic = lda_model.doc_topic_\n",
    "lda_keys = []\n",
    "for i, logLine in enumerate(genVectorsFromThis):\n",
    "    lda_keys += [doc_topic[i].argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_lda = bp.figure(plot_width=800, plot_height=800, title=\"loglines (LDA)\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "    x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "plot_lda.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], \n",
    "                 color=colormap[lda_keys][:10000], \n",
    "                 source=bp.ColumnDataSource({\n",
    "                    \"LogLine\": allOriginal[:10000],\n",
    "                    \"LineProcessed\":genVectorsFromThis[:1000],\n",
    "                    \"topic_key\": lda_keys[:10000]\n",
    "                }))\n",
    "hover = plot_lda.select(dict(type=HoverTool))\n",
    "hover.tooltips={\"LogLine\": \"@LogLine -PROCESSED:[@LineProcessed) -topic: @topic_key)\"}\n",
    "show(plot_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
