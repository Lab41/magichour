{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auditd to transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "import re\n",
    "type_lookup_table = {u'ADD_GROUP': 4,\n",
    " u'ADD_USER': 12,\n",
    " u'ANOM_ABEND': 0,\n",
    " u'CONFIG_CHANGE': 24,\n",
    " u'CRED_ACQ': 20,\n",
    " u'CRED_DISP': 13,\n",
    " u'CRED_REFR': 17,\n",
    " u'CRYPTO_KEY_USER': 6,\n",
    " u'CRYPTO_SESSION': 14,\n",
    " u'DAEMON_END': 8,\n",
    " u'DAEMON_START': 7,\n",
    " u'LOGIN': 19,\n",
    " u'NETFILTER_CFG': 22,\n",
    " u'SYSCALL': 5,\n",
    " u'SYSTEM_RUNLEVEL': 1,\n",
    " u'SYSTEM_SHUTDOWN': 18,\n",
    " u'USER_ACCT': 9,\n",
    " u'USER_AUTH': 10,\n",
    " u'USER_CHAUTHTOK': 21,\n",
    " u'USER_CMD': 3,\n",
    " u'USER_END': 23,\n",
    " u'USER_ERR': 11,\n",
    " u'USER_LOGIN': 2,\n",
    " u'USER_LOGOUT': 15,\n",
    " u'USER_START': 16}\n",
    "def get_data(line, window_size=10, start_time=1422496861):\n",
    "    timestamp = float(re.search('audit\\(([0-9]+.[0-9]+)', line).group(1))\n",
    "    type_code = type_lookup_table[re.search('type=([A-Z_]+) ', line).group(1)]\n",
    "    window = int((timestamp -start_time)/window_size)\n",
    "    return (window, type_code)\n",
    "\n",
    "from collections import defaultdict\n",
    "def get_longest_sets_possible(input_sets):\n",
    "    def is_subset(main_set, item):\n",
    "        is_subset = False\n",
    "        for main_item in main_set:\n",
    "            if item.issubset(main_item):\n",
    "                is_subset = True\n",
    "        return is_subset\n",
    "    input_dict = defaultdict(set)\n",
    "    for i in input_sets:\n",
    "        input_dict[len(i)].add(i)\n",
    "    \n",
    "    output_sets = set()\n",
    "    lengths = sorted(input_dict.keys(), reverse=True) # Largest first\n",
    "    for i in input_dict[lengths[0]]: # since they are all the longest length we know that they are good\n",
    "        output_sets.add(i) \n",
    "    \n",
    "    for length in lengths[1:]:\n",
    "        for item in input_dict[length]:\n",
    "            if not is_subset(output_sets, item):\n",
    "                output_sets.add(item)\n",
    "    return output_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "#logs = sc.textFile(\"hdfs:///user/ytesfaye/lab41_logs_small.log.gz\").repartition(10)\n",
    "#transactions = logs.map(get_data) \\\n",
    "#                   .groupByKey() \\\n",
    "#                   .map(lambda (key, iterator): list(set(iterator)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tbird_logs = sc.textFile(\"hdfs://namenode/magichour/tbird.log.preProc.stringmatch\").repartition(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tbird_logs.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tbird_logs.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#tBird Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "#tbird_logs = sc.textFile(\"hdfs://namenode/user/ytesfaye/tbird.log.out.logCluster.processed.gz\").repartition(10)\n",
    "tbird_logs = sc.textFile(\"hdfs://namenode/magichour/tbird.log.preProc.stringmatch\").repartition(10)\n",
    "#transactionsNoFreqs = sc.textFile(\"hdfs://namenode/magichour/tbird.log.out.logCluster.processed.10sec.50000freqnew\")\n",
    "\n",
    "def get_tbird_data(line, window_size=10):\n",
    "    ls = line.split(',')\n",
    "    timestamp = float(ls[0])\n",
    "    type_code = int(ls[1])\n",
    "    window = int(timestamp/window_size)\n",
    "    #ls = line.split(' ')\n",
    "    #return ls\n",
    "    return (window, type_code)\n",
    "\n",
    "transactions = tbird_logs.map(get_tbird_data) \\\n",
    "                   .groupByKey() \\\n",
    "                   .map(lambda (key, iterator): list(set([item for item in iterator if item != -1]))) \\\n",
    "                   .filter(lambda x: len(x) >= 2)\n",
    "\n",
    "#transactions = transactionsNoFreqs.map(get_tbird_data)\n",
    "\n",
    "# Load lookup table so that we can get back to raw strings\n",
    "template_lookup = {}\n",
    "for line in sc.textFile(\"hdfs://namenode/user/ytesfaye/tmp.txt\").collect():\n",
    "    ls = line.split(',', 2)\n",
    "    template_lookup[int(ls[0])] = ls[1]\n",
    "dimension = max(template_lookup.keys()) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Lib LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors, SparseVector\n",
    "import numpy as np\n",
    "from scipy.sparse import lil_matrix \n",
    "def make_vector(input_list, dimension=dimension):\n",
    "    input_list, key = input_list\n",
    "    return [key, SparseVector(dimension, sorted(input_list), np.ones(len(input_list)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorized_transactions = transactions.filter(lambda x: len(x) >= 2).zipWithUniqueId().map(make_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.clustering import LDA\n",
    "model = LDA.train(vectorized_transactions, k=5, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topics = model.topicsMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_topics = 20\n",
    "num_words_per_topic = 5\n",
    "for topic_num, (ids, weights) in enumerate(model.describeTopics(num_words_per_topic)):\n",
    "    print 'Topic %d'%topic_num\n",
    "    print '---------------------'\n",
    "    for i, n in enumerate(ids):\n",
    "        print '%4d (%2.2f): %s'%(n, weights[i]*100.0, template_lookup[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ML Lib FP-Growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.fpm import FPGrowth\n",
    "model = FPGrowth.train(transactions, minSupport=0.01, numPartitions=10)\n",
    "result = model.freqItemsets().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "items = [frozenset(fi.items) for fi in result]\n",
    "pruned_items = list(get_longest_sets_possible(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(pruned_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for item in pruned_items:\n",
    "    print ''.join([' ' + str(i) + ' ' for i in sorted(item, key=int)])\n",
    "    \n",
    "\n",
    "#for i in range(len(pruned_items)):\n",
    "    #print '---------------------'\n",
    "    #for template in pruned_items[i]:\n",
    "        #print '%4d'%template, template_lookup[template]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "items = [frozenset(fi.items) for fi in result]\n",
    "pruned_items = list(get_longest_sets_possible(items))\n",
    "for item in pruned_items:\n",
    "    print '|'.join([',' + str(i) + ',' for i in sorted(item, key=int)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
