{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "from collections import namedtuple\n",
    "from os import listdir\n",
    "\n",
    "Entry = namedtuple('Entry',['value','cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files =  listdir('/Users/dgrossman/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filelist = list()\n",
    "for f in files:\n",
    "    if f.endswith('.out') and f.startswith('tbirdOverlap'):\n",
    "        filelist.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = dict()\n",
    "for f in filelist:\n",
    "    a = open('/Users/dgrossman/data/' + f,'r')\n",
    "    words = \"\"\n",
    "    for w in a.readlines():\n",
    "        words += w.lstrip().strip()\n",
    "        words += ' '\n",
    "    a.close()\n",
    "    documents[f] = words.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_set = list()\n",
    "for d in documents.itervalues():\n",
    "    doc_set.append(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = []\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for d in doc_set:\n",
    "    tokens = tokenizer.tokenize(d)\n",
    "    texts.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word = dictionary, passes=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outData = list()\n",
    "for x in(ldamodel.print_topics(num_topics=10, num_words=40)):\n",
    "    parts =  x[1].split('+')\n",
    "    sum = 0\n",
    "    lines = list()\n",
    "    empty = True\n",
    "    # print x\n",
    "    for p in parts:\n",
    "        val,cluster = p.lstrip().strip().split('*')\n",
    "        # print val,cluster\n",
    "        \n",
    "        if (sum + float(val) < .8) or empty:\n",
    "            sum += float(val)\n",
    "            # print 'adding:',cluster\n",
    "            empty = False\n",
    "            lines.append(Entry(float(val),int(cluster)))\n",
    "    #print sum,lines\n",
    "    #print\n",
    "    outData.append((sum,lines))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "templateFile = '/Users/dgrossman/data/tbird.Clusters'\n",
    "tf = open(templateFile,'r').readlines()\n",
    "templateList = list()\n",
    "for t in tf:\n",
    "    # print t.strip()\n",
    "    unescaped= re.sub(r'[\\^]','',re.sub(r'[\\\\]', '', t)).strip()\n",
    "    templateList.append(unescaped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for topic, X in enumerate(outData):\n",
    "    s = X[0]\n",
    "    ent = X[1]\n",
    "    print 'topic=%i|count=%i|energy=%f' % (topic, len(ent), s)\n",
    "    wordBag = set()\n",
    "    for e in ent:\n",
    "        cluster, string = templateList[e.cluster].split(',',1)\n",
    "        print '\\t%5i| %1.4f| %s' % (int(cluster),e.value,string)\n",
    "        for s in string.split():\n",
    "            wordBag.add(s)\n",
    "    print\n",
    "    print 'words used : %s' % ' '.join(sorted(wordBag))\n",
    "    print\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
